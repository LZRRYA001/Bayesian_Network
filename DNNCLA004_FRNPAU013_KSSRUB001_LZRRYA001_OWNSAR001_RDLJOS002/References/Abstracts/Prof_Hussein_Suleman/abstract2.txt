With the demand for recorded lectures to be made available as soon as possible, the University of Cape Town (UCT) needs to find innovative ways of removing bottlenecks in lecture capture workflow and thereby improving turn-around times from capture to publication. UCT utilises Opencast, which is an open source system to manage all the steps  in  the  lecture-capture  process.  One  of  the  steps  involves  manual  trimming  of  unwanted segments from the beginning and end of video before it is published. These segments generally contain student chatter. The trimming step of the lecture-capture process has been identified as a bottleneck due to its dependence on staff availability. In this study, we investigate the potential of audio classification to automate this step. A  classification  model  was  trained  to  detect  two  classes:  speech  and  non-speech. Speech represents a single dominant voice, for example, the lecturer, and non-speech represents  student  chatter,  silence  and  other  environmental  sounds.  In  conjunction  with the classification model, the first and last instances of the speech class together with  their  timestamps  are  detected.  These  timestamps  are  used  to  predict  the  start  and end trim points for the recorded lecture. The  classification  model  achieved  a  97.8%  accuracy  rate  at  detecting  speech  from  non-speech.  The  start  trim  point  predictions  were  very  positive,  with  an  average  difference of -11.22s  from  gold  standard  data.  End  trim point  predictions  showed  a  much  greater  deviation,  with  an  average  difference  of  145.16s  from  gold  standard  data.   Discussions   between   the   lecturer   and   students,     after   the   lecture,   was   predominantly the reason for this discrepancy.